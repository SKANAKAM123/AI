{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Iqbg5jyUOL4D",
   "metadata": {
    "id": "Iqbg5jyUOL4D"
   },
   "outputs": [],
   "source": [
    "# setting sumo path in the colab\n",
    "%env SUMO_HOME=C:\\Program Files (x86)\\Eclipse\\Sumo\n",
    "import os\n",
    "os.environ['PATH'] += \":/usr/bin\"\n",
    "!apt-get -qq update -y\n",
    "!apt-get -qq upgrade -y\n",
    "!apt-get -qq install cmake libxerces-c-dev libfox-1.6-dev libgdal-dev libproj-dev libgl2ps-dev swig -y\n",
    "!pip install -q --upgrade \"jupyterlab>=3.0\" jupyterlab-dash==0.1.0a3\n",
    "!apt-get -qq install sumo sumo-tools sumo-doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3305d1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dd3305d1",
    "outputId": "3639e589-968a-4aa5-a46b-fcb116dd6ace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting traci\n",
      "  Downloading traci-1.17.0-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.1/268.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sumolib>=1.17.0\n",
      "  Downloading sumolib-1.17.0-py3-none-any.whl (146 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.8/146.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sumolib, traci\n",
      "Successfully installed sumolib-1.17.0 traci-1.17.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "from shutil import copyfile\n",
    "import sys\n",
    "import math \n",
    "! pip install traci\n",
    "import traci\n",
    "import numpy as np\n",
    "import random\n",
    "import timeit\n",
    "\n",
    "from generator import TrafficGenerator\n",
    "from memory import Memory     ## Prority Experience Memory \n",
    "from visualization import Visualization\n",
    "from utils import import_train_configuration,set_sumo, set_train_path,get_model_path\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0570c094",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0570c094",
    "outputId": "e2585a05-a533-42ce-f8f9-cf3028e1df08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting d2l\n",
      "  Downloading d2l-0.17.6-py3-none-any.whl (112 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.6/112.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas==1.2.4\n",
      "  Downloading pandas-1.2.4-cp39-cp39-manylinux1_x86_64.whl (9.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting numpy==1.21.5\n",
      "  Downloading numpy-1.21.5-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting matplotlib==3.5.1\n",
      "  Downloading matplotlib-3.5.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m78.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jupyter==1.0.0\n",
      "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
      "Collecting requests==2.25.1\n",
      "  Downloading requests-2.25.1-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: notebook in /usr/local/lib/python3.9/dist-packages (from jupyter==1.0.0->d2l) (6.4.8)\n",
      "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.9/dist-packages (from jupyter==1.0.0->d2l) (6.1.0)\n",
      "Collecting qtconsole\n",
      "  Downloading qtconsole-5.4.2-py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.2/121.2 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nbconvert in /usr/local/lib/python3.9/dist-packages (from jupyter==1.0.0->d2l) (6.5.4)\n",
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.9/dist-packages (from jupyter==1.0.0->d2l) (7.7.1)\n",
      "Requirement already satisfied: ipykernel in /usr/local/lib/python3.9/dist-packages (from jupyter==1.0.0->d2l) (5.5.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (1.4.4)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (8.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (23.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (4.39.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib==3.5.1->d2l) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas==1.2.4->d2l) (2022.7.1)\n",
      "Collecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests==2.25.1->d2l) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests==2.25.1->d2l) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests==2.25.1->d2l) (1.26.15)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.1->d2l) (1.16.0)\n",
      "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.2)\n",
      "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.7.1)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (6.1.12)\n",
      "Requirement already satisfied: ipython>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (7.34.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.0.7)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.4)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (3.0.38)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from jupyter-console->jupyter==1.0.0->d2l) (2.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (2.1.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
      "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (3.1.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.3)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.3.0)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.2.2)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.9.2)\n",
      "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.2.1)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (6.0.0)\n",
      "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.8.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (4.11.2)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.9/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.4)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.17.1)\n",
      "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter==1.0.0->d2l) (23.2.1)\n",
      "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.16.0)\n",
      "Requirement already satisfied: nest-asyncio>=1.5 in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.5.6)\n",
      "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.9/dist-packages (from notebook->jupyter==1.0.0->d2l) (21.3.0)\n",
      "Collecting qtpy>=2.0.1\n",
      "  Downloading QtPy-2.3.1-py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting jedi>=0.16\n",
      "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (4.4.2)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.1.6)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (67.7.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.9/dist-packages (from ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (4.8.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core>=4.7->nbconvert->jupyter==1.0.0->d2l) (3.2.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (4.3.3)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (2.16.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->jupyter-console->jupyter==1.0.0->d2l) (0.2.6)\n",
      "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.9/dist-packages (from terminado>=0.8.3->notebook->jupyter==1.0.0->d2l) (0.7.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.9/dist-packages (from argon2-cffi->notebook->jupyter==1.0.0->d2l) (21.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.9/dist-packages (from beautifulsoup4->nbconvert->jupyter==1.0.0->d2l) (2.4.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.9/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.16->ipython>=5.0.0->ipykernel->jupyter==1.0.0->d2l) (0.8.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (23.1.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert->jupyter==1.0.0->d2l) (0.19.3)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter==1.0.0->d2l) (2.21)\n",
      "Installing collected packages: qtpy, numpy, jedi, idna, requests, pandas, matplotlib, qtconsole, jupyter, d2l\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.22.4\n",
      "    Uninstalling numpy-1.22.4:\n",
      "      Successfully uninstalled numpy-1.22.4\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.27.1\n",
      "    Uninstalling requests-2.27.1:\n",
      "      Successfully uninstalled requests-2.27.1\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.5.3\n",
      "    Uninstalling pandas-1.5.3:\n",
      "      Successfully uninstalled pandas-1.5.3\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.1\n",
      "    Uninstalling matplotlib-3.7.1:\n",
      "      Successfully uninstalled matplotlib-3.7.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yfinance 0.2.18 requires pandas>=1.3.0, but you have pandas 1.2.4 which is incompatible.\n",
      "yfinance 0.2.18 requires requests>=2.26, but you have requests 2.25.1 which is incompatible.\n",
      "xarray 2022.12.0 requires pandas>=1.3, but you have pandas 1.2.4 which is incompatible.\n",
      "tweepy 4.13.0 requires requests<3,>=2.27.0, but you have requests 2.25.1 which is incompatible.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.21.5 which is incompatible.\n",
      "plotnine 0.10.1 requires pandas>=1.3.5, but you have pandas 1.2.4 which is incompatible.\n",
      "mizani 0.8.1 requires pandas>=1.3.5, but you have pandas 1.2.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas~=1.5.3, but you have pandas 1.2.4 which is incompatible.\n",
      "google-colab 1.0.0 requires requests>=2.27.0, but you have requests 2.25.1 which is incompatible.\n",
      "arviz 0.15.1 requires pandas>=1.3.0, but you have pandas 1.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed d2l-0.17.6 idna-2.10 jedi-0.18.2 jupyter-1.0.0 matplotlib-3.5.1 numpy-1.21.5 pandas-1.2.4 qtconsole-5.4.2 qtpy-2.3.1 requests-2.25.1\n"
     ]
    }
   ],
   "source": [
    "! pip install d2l\n",
    "from d2l import torch as d2l  # Transformer block \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "937e5697",
   "metadata": {
    "id": "937e5697"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b273f0",
   "metadata": {
    "id": "95b273f0"
   },
   "source": [
    "## Your Hyperparameter Setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467454e0",
   "metadata": {
    "id": "467454e0"
   },
   "outputs": [],
   "source": [
    "# [simulation]\n",
    "gui = False\n",
    "total_episodes = 5\n",
    "max_steps = 3600\n",
    "n_cars_generated = 1000\n",
    "green_duration = 7\n",
    "yellow_duration = 4\n",
    "\n",
    "# [model]\n",
    "num_layers = 4\n",
    "width_layers = 800\n",
    "batch_size = 64\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 512\n",
    "update_epochs = 10\n",
    "num_atoms = 51\n",
    "Vmin = -10\n",
    "Vmax = 10\n",
    "\n",
    "# [memory]\n",
    "memory_size_min = 1000\n",
    "memory_size_max = 50000\n",
    "\n",
    "# [agent]\n",
    "num_states = 1400\n",
    "num_actions = 4\n",
    "gamma = 0.9\n",
    "\n",
    "# [dir] \n",
    "model_to_test = 1   # load model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b13ef30",
   "metadata": {
    "id": "5b13ef30"
   },
   "source": [
    "## Your Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16fa1d5a",
   "metadata": {
    "id": "16fa1d5a"
   },
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.4):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        \n",
    "        self.in_features  = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init     = std_init\n",
    "        \n",
    "        self.weight_mu    = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n",
    "        \n",
    "        self.bias_mu    = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.training: \n",
    "            weight = self.weight_mu + self.weight_sigma.mul(Variable(self.weight_epsilon))\n",
    "            bias   = self.bias_mu   + self.bias_sigma.mul(Variable(self.bias_epsilon))\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias   = self.bias_mu\n",
    "        \n",
    "        return F.linear(x, weight, bias)\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / math.sqrt(self.weight_mu.size(1))\n",
    "        \n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.weight_sigma.size(1)))\n",
    "        \n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.bias_sigma.size(0)))\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        epsilon_in  = self._scale_noise(self.in_features)\n",
    "        epsilon_out = self._scale_noise(self.out_features)\n",
    "        \n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(self._scale_noise(self.out_features))\n",
    "    \n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        x = x.sign().mul(x.abs().sqrt())\n",
    "        return x\n",
    "\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if torch.cuda.is_available() else autograd.Variable(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67b5e711",
   "metadata": {
    "id": "67b5e711"
   },
   "outputs": [],
   "source": [
    "## Categorical & Dueling DQN \n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size,num_layers,num_atoms):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.num_atoms = num_atoms\n",
    "        self.support = torch.linspace(Vmin, Vmax, num_atoms).to(device) \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 16, (1,3), (1,2), padding=(0,1))\n",
    "        # self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(16, 32, (2,3), (2,2), padding=(0,1))\n",
    "        self.linear_c = nn.Linear(32*7*25, 4096)\n",
    "        self.linear_p = nn.Linear(4,10) \n",
    "        self.linear1 = nn.Linear(4096, 2048)\n",
    "        self.linear2 = NoisyLinear(2068, 1024)   \n",
    "        self.linear3 = NoisyLinear(2068, 1024)\n",
    "        self.linear_A = NoisyLinear(1024, self.output_size*self.num_atoms)\n",
    "        self.linear_V = NoisyLinear(1024, 1*self.num_atoms)\n",
    "    \n",
    "    def forward(self, s,a,b):\n",
    "        dist = self.dist(s,a,b)\n",
    "        q = torch.sum(dist*self.support, dim=2)\n",
    "        return q\n",
    "    \n",
    "    def dist(self, s,a,b):\n",
    "        x = F.relu(self.conv2(F.relu(self.conv1(s))))\n",
    "        x = x.view(-1, 32*7*25)\n",
    "        x = F.relu(self.linear1(F.relu(self.linear_c(x)))) \n",
    "        a = torch.tensor(a,dtype=torch.float).view(-1,4)\n",
    "        b = torch.tensor(b,dtype=torch.float).view(-1,4)\n",
    "        a = F.relu(self.linear_p(a)).view(-1,10)\n",
    "        b = F.relu(self.linear_p(b)).view(-1,10) \n",
    "        x = torch.cat([x,a,b],dim=1).view(-1,2068)\n",
    "        V = self.linear_V(F.relu(self.linear2(x)))\n",
    "        A = self.linear_A(F.relu(self.linear3(x)))\n",
    "        V = V.view(-1,1,self.num_atoms)\n",
    "        A = A.view(-1,self.output_size,self.num_atoms)\n",
    "        q = V + A - A.mean(dim=1, keepdim=True)\n",
    "        \n",
    "        dist = F.softmax(q,dim=-1)\n",
    "        dist = dist.clamp(min=1e-3) # for avoiding nans \n",
    "        return dist\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        self.linear2.reset_noise()\n",
    "        self.linear3.reset_noise()\n",
    "        self.linear_A.reset_noise()\n",
    "        self.linear_V.reset_noise()\n",
    "        \n",
    "    # def feature_size(self):\n",
    "        # return self.conv2(self.conv1(torch.zeros(1, *self.input_size))).view(1, -1).size(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7998e2",
   "metadata": {
    "id": "ab7998e2"
   },
   "source": [
    "- Paper link: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf32a236",
   "metadata": {
    "id": "cf32a236"
   },
   "source": [
    "## Phase Setting based on Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2483dec",
   "metadata": {
    "id": "e2483dec"
   },
   "outputs": [],
   "source": [
    "# phase codes based on SUMO environment.net.xml \n",
    "PHASE_NS_GREEN = 0  # action 0 for Variable Order\n",
    "PHASE_NS_YELLOW = 1\n",
    "PHASE_NSL_GREEN = 2  # action 1 for Variable Order\n",
    "PHASE_NSL_YELLOW = 3\n",
    "PHASE_EW_GREEN = 4  # action 2 for Variable Order\n",
    "PHASE_EW_YELLOW = 5\n",
    "PHASE_EWL_GREEN = 6  # action 3 for Variable Order\n",
    "PHASE_EWL_YELLOW = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27991183",
   "metadata": {
    "id": "27991183"
   },
   "source": [
    "## Your State Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acc7a6ee",
   "metadata": {
    "id": "acc7a6ee"
   },
   "outputs": [],
   "source": [
    "def _get_state():\n",
    "    \"\"\"\n",
    "    Retrieve the state of the intersection from sumo, in the form of cell occupancy\n",
    "    \"\"\"\n",
    "    #state = np.zeros((56))        ## Vector \n",
    "    state = np.zeros((3,14,100))   ## DTSE \n",
    "    \n",
    "    lane = [\"N2TL_0\",\"N2TL_1\",\"N2TL_2\",\"E2TL_0\",\"E2TL_1\",\"E2TL_2\",\"E2TL_3\",\"S2TL_0\",\"S2TL_1\",\"S2TL_2\",\"W2TL_0\",\"W2TL_1\",\"W2TL_2\",\"W2TL_3\"]\n",
    "    car_list = traci.vehicle.getIDList()\n",
    "    lane_group = 0\n",
    "\n",
    "    for car_id in car_list:\n",
    "        lane_pos = traci.vehicle.getLanePosition(car_id)\n",
    "        car_speed = traci.vehicle.getSpeed(car_id)\n",
    "        lane_id = traci.vehicle.getLaneID(car_id)\n",
    "        # inversion of lane pos, so if the car is close to the traffic light -> lane_pos = 0 --- 750 = max len of a road\n",
    "        lane_pos = 750 - lane_pos\n",
    "        \n",
    "        # distance in meters from the traffic light -> mapping into cells\n",
    "        lane_cell = int(lane_pos/7.5)\n",
    "\n",
    "        for i in range(len(lane)):\n",
    "            if lane_id == lane[i]:\n",
    "                lane_group = i\n",
    "\n",
    "        state[0][lane_group][lane_cell] = 1\n",
    "        state[1][lane_group][lane_cell] = car_speed\n",
    "        state[2][lane_group][lane_cell] = traci.vehicle.getAccumulatedWaitingTime(car_id)\n",
    "\n",
    "        #if lane_cell <= 10: \n",
    "        #    state[lane_group] += 1\n",
    "        #state[lane_group+14] += 1\n",
    "        #state[lane_group+28] += car_speed\n",
    "        #state[lane_group+42] += traci.vehicle.getAccumulatedWaitingTime(car_id)\n",
    "        \n",
    "    return state.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffebf6e",
   "metadata": {
    "id": "dffebf6e"
   },
   "source": [
    "## Your Action Definition "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c1d8be0",
   "metadata": {
    "id": "3c1d8be0"
   },
   "source": [
    "#Duration_NS = 12\n",
    "#Duration_NSL = 9\n",
    "#Duration_EW = 16\n",
    "#Duration_EWL = 7\n",
    "\n",
    "Duration_NS = 18\n",
    "Duration_NSL = 7\n",
    "Duration_EW = 16\n",
    "Duration_EWL = 7\n",
    "\n",
    "# Duration_NS = 30\n",
    "# Duration_NSL = 11\n",
    "# Duration_EW = 22\n",
    "# Duration_EWL = 12\n",
    "\n",
    "O = [Duration_NS,Duration_NSL,Duration_EW,Duration_EWL]\n",
    "duration = [Duration_NS,Duration_NSL,Duration_EW,Duration_EWL]\n",
    "old_duration = [Duration_NS,Duration_NSL,Duration_EW,Duration_EWL]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2719e984",
   "metadata": {
    "id": "2719e984"
   },
   "source": [
    "# 1. Simulation Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9b6d23e",
   "metadata": {
    "id": "b9b6d23e"
   },
   "outputs": [],
   "source": [
    "class Simulation:\n",
    "    def __init__(self, Model, Memory, TrafficGen, sumo_cmd, gamma, max_steps, green_duration, yellow_duration, num_states, num_actions):\n",
    "        self._model = Model\n",
    "        self._Model_A = Model.critic\n",
    "        self._Model_B = Model.critic_target\n",
    "        self._Memory = Memory\n",
    "        self._TrafficGen = TrafficGen\n",
    "        self._gamma = gamma\n",
    "        self._step = 0\n",
    "        self._sumo_cmd = sumo_cmd\n",
    "        self._max_steps = max_steps \n",
    "        self._green_duration = green_duration\n",
    "        self._yellow_duration = yellow_duration\n",
    "        self._num_states = num_states\n",
    "        self._num_actions = num_actions\n",
    "        self._reward_store = []\n",
    "        self._speed_store = []\n",
    "        self._cumulative_wait_store = []\n",
    "        self._avg_queue_length_store = []\n",
    "\n",
    "    def run(self, episode, epsilon):\n",
    "        \"\"\"\n",
    "        Runs an episode of simulation, then starts a training session\n",
    "        \"\"\"\n",
    "        self.training = False\n",
    "        start_time = timeit.default_timer()\n",
    "\n",
    "        # first, generate the route file for this simulation and set up sumo\n",
    "        self._TrafficGen.generate_routefile(seed=episode)\n",
    "        traci.start(self._sumo_cmd)\n",
    "        print(\"Simulating...\")\n",
    "\n",
    "        # inits\n",
    "        self._step = 0\n",
    "        self._waiting_times = {}\n",
    "        self._sum_neg_reward = 0\n",
    "        self._sum_queue_length = 0\n",
    "        self._sum_waiting_time = 0\n",
    "        self._sum_speed = 0\n",
    "        old_total_wait = 0\n",
    "        old_action = 0\n",
    "        reward = 0\n",
    "        re = 0\n",
    "        current_phase = 0\n",
    "        self.reward = 0\n",
    "        duration = [0,0,0,0]\n",
    "        old_duration = [0,0,0,0]\n",
    "        old_duration_2 = [0,0,0,0]\n",
    "        old_duration_3 = [0,0,0,0]\n",
    "        old_action_2 = 0\n",
    "        old_action_3 = 0\n",
    "        reward_1 = 0\n",
    "        reward_2 = 0\n",
    "        x = [0,0,0,0]\n",
    "        x_1 = [0,0,0,0]\n",
    "        x_2 = [0,0,0,0]\n",
    "        x_3 = [0,0,0,0]\n",
    "        \n",
    "        self._simulate(50)  ## Warm Environment\n",
    "        old_state = _get_state()\n",
    "        old_state_2 = _get_state()\n",
    "        old_state_3 = _get_state()\n",
    "\n",
    "        while self._step < self._max_steps:\n",
    "            \n",
    "            # get current state of the intersection\n",
    "            current_state = _get_state()\n",
    "\n",
    "            # calculate reward of previous action: \n",
    "            # waiting time = seconds waited by a car since the spawn in the environment, cumulated for every car in incoming lanes\n",
    "            current_total_wait = self._collect_waiting_times()    \n",
    "            \n",
    "            ## Your Reward Function \n",
    "            reward = - current_total_wait\n",
    "            \n",
    "            ## (change in cumulative waiting time between actions) is not suitable for Multi-step TD learning \n",
    "            # reward = old_total_wait - current_total_wait           \n",
    "    \n",
    "            current_phase = int(traci.trafficlight.getPhase(\"TL\")/2)\n",
    "            phase = current_phase - 1\n",
    "            if phase == -1:\n",
    "                phase = 3\n",
    "                \n",
    "            action = self._choose_action(current_state, epsilon,current_phase,duration) # phase, epsilon) \n",
    "            \n",
    "            # print(phase, old_action, old_duration, duration)\n",
    "            # saving the data into the memory\n",
    "            if self._step != 0:\n",
    "                x = [0,0,0,0]\n",
    "                y = [0,0,0,0]\n",
    "                x[old_action] = 1\n",
    "                y[action] = 1         \n",
    "                \n",
    "                if self._step < self._max_steps - self._green_duration - self._yellow_duration:\n",
    "                    TD = self._model.get_TD(old_state_3,old_action_3,reward_2,reward_1,reward,current_state,0,x_3,y,old_duration_3,duration).squeeze(0).detach().tolist()\n",
    "                    self._Memory.add_sample(TD, (old_state_3,old_action_3,reward_2,reward_1,reward,current_state,0,x_3,y,old_duration_3,duration))\n",
    "                else:\n",
    "                    TD = self._model.get_TD(old_state_3,old_action_3,reward_2,reward_1,reward,current_state,1,x_3,y,old_duration_3,duration).squeeze(0).detach().tolist()\n",
    "                    self._Memory.add_sample(TD, (old_state_3,old_action_3,reward_2,reward_1,reward,current_state,1,x_3,y,old_duration_3,duration))\n",
    "\n",
    "            # if the chosen phase is different from the last phase, activate the yellow phase\n",
    "            if self._step != 0 and old_action != action:# and i == 0:\n",
    "                self._set_yellow_phase(current_phase)\n",
    "                self._simulate(self._yellow_duration)\n",
    "                duration[action] = self._green_duration\n",
    "            else: \n",
    "                duration[action] += 7\n",
    "                \n",
    "            self._set_green_phase(action)\n",
    "            self._simulate(self._green_duration)\n",
    "            # print(\"current phase:\",current_phase,\"green:\",a)\n",
    "\n",
    "            # saving variables for later & accumulate reward\n",
    "            old_state_3 = old_state_2\n",
    "            old_state_2 = old_state\n",
    "            old_state = current_state\n",
    "            old_action_3 = old_action_2\n",
    "            old_action_2 = old_action\n",
    "            old_action = action\n",
    "            x_3 = x_2\n",
    "            x_2 = x_1\n",
    "            x_1 = x\n",
    "            old_duration_3 = old_duration_2\n",
    "            old_duration_2 = old_duration\n",
    "            old_duration = duration\n",
    "            old_total_wait = current_total_wait\n",
    "            reward_2 = reward_1\n",
    "            reward_1 = reward\n",
    "\n",
    "            # saving only the meaningful reward to better see if the agent is behaving correctly\n",
    "            # if reward < 0:\n",
    "            self._sum_neg_reward += reward\n",
    "            re += 1\n",
    "            self.reward = self._sum_neg_reward/re\n",
    "                \n",
    "        print(\"Total Queue:\",self._sum_queue_length, \"  \", \"Average Reward:\", self.reward, \" \", \"Average Speed:\",self._sum_speed/self._max_steps)\n",
    "\n",
    "        self._save_episode_stats()\n",
    "        print(\"Total Reward:\", self._sum_neg_reward, \"- Epsilon:\", round(epsilon, 2))\n",
    "        traci.close()\n",
    "        simulation_time = round(timeit.default_timer() - start_time, 1)\n",
    "\n",
    "        return simulation_time\n",
    "\n",
    "\n",
    "    def _simulate(self, steps_todo):\n",
    "        \"\"\"\n",
    "        Execute steps in sumo while gathering statistics\n",
    "        \"\"\"\n",
    "        if (self._step + steps_todo) >= self._max_steps:  # do not do more steps than the maximum allowed number of steps\n",
    "            steps_todo = self._max_steps - self._step\n",
    "\n",
    "        while steps_todo > 0:\n",
    "            traci.simulationStep()  # simulate 1 step in sumo\n",
    "            self._step += 1 # update the step counter\n",
    "            steps_todo -= 1\n",
    "            queue_length = self._get_queue_length()\n",
    "            self._sum_queue_length += queue_length\n",
    "            self._sum_waiting_time += queue_length # 1 step while wating in queue means 1 second waited, for each car, therefore queue_lenght == waited_seconds\n",
    "            speed = self._get_speed()\n",
    "            self._sum_speed += speed\n",
    "        \n",
    "    def _collect_waiting_times(self):            # For reward \n",
    "        \"\"\"\n",
    "        Retrieve the waiting time of every car in the incoming roads\n",
    "        \"\"\"\n",
    "        incoming_roads = [\"E2TL\", \"N2TL\", \"W2TL\", \"S2TL\"]\n",
    "        car_list = traci.vehicle.getIDList()\n",
    "        self._waiting_times = {}\n",
    "        for car_id in car_list:\n",
    "            wait_time = traci.vehicle.getAccumulatedWaitingTime(car_id)\n",
    "            road_id = traci.vehicle.getRoadID(car_id)  # get the road id where the car is located\n",
    "            if road_id in incoming_roads:  # consider only the waiting times of cars in incoming roads\n",
    "                self._waiting_times[car_id] = wait_time \n",
    "            else:\n",
    "                if car_id in self._waiting_times: # a car that was tracked has cleared the intersection\n",
    "                    del self._waiting_times[car_id] \n",
    "                \n",
    "        if len(self._waiting_times) == 0: \n",
    "            total_waiting_time = 0\n",
    "        else: \n",
    "            total_waiting_time = sum(self._waiting_times.values())/len(self._waiting_times)\n",
    "        return total_waiting_time \n",
    "\n",
    "    def _choose_action(self, state, epsilon,phase,old_duration): #phase, epsilon):\n",
    "        \"\"\"\n",
    "        Decide wheter to perform an explorative or exploitative action, according to an epsilon-greedy policy\n",
    "        \"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self._num_actions - 1) # random action\n",
    "        else:\n",
    "            x = [0,0,0,0]\n",
    "            x[phase] = 1\n",
    "            return torch.argmax(self._model.predict(state,x,old_duration))#,phase)) # the best action given the current state\n",
    "\n",
    "    def _set_yellow_phase(self, old_action):\n",
    "        \"\"\"\n",
    "        Activate the correct yellow light combination in sumo\n",
    "        \"\"\"\n",
    "        yellow_phase_code = old_action * 2 + 1 # obtain the yellow phase code, based on the old action (ref on environment.net.xml)\n",
    "        traci.trafficlight.setPhase(\"TL\", yellow_phase_code)\n",
    "\n",
    "    def _set_green_phase(self, action_number):   ## For Variable Order Method \n",
    "        \"\"\"\n",
    "        Activate the correct green light combination in sumo\n",
    "        \"\"\"\n",
    "        if action_number == 0:\n",
    "            traci.trafficlight.setPhase(\"TL\", PHASE_NS_GREEN)\n",
    "        elif action_number == 1:\n",
    "            traci.trafficlight.setPhase(\"TL\", PHASE_NSL_GREEN)\n",
    "        elif action_number == 2:\n",
    "            traci.trafficlight.setPhase(\"TL\", PHASE_EW_GREEN)\n",
    "        elif action_number == 3:\n",
    "            traci.trafficlight.setPhase(\"TL\", PHASE_EWL_GREEN)\n",
    "\n",
    "    def _get_green(self,current_phase):       ## For Finetuning Method \n",
    "        if current_phase == 0:\n",
    "            green = Duration_NS\n",
    "        elif current_phase == 1:\n",
    "            green = Duration_NSL\n",
    "        elif current_phase == 2:\n",
    "            green = Duration_EW\n",
    "        else: \n",
    "            green = Duration_EWL\n",
    "        return green\n",
    "\n",
    "    def _get_queue_length(self):          # For evaluation \n",
    "        \"\"\"\n",
    "        Retrieve the number of cars with speed = 0 in every incoming lane\n",
    "        \"\"\"\n",
    "        halt_N = traci.edge.getLastStepHaltingNumber(\"N2TL\")\n",
    "        halt_S = traci.edge.getLastStepHaltingNumber(\"S2TL\")\n",
    "        halt_E = traci.edge.getLastStepHaltingNumber(\"E2TL\")\n",
    "        halt_W = traci.edge.getLastStepHaltingNumber(\"W2TL\")\n",
    "        queue_length = halt_N + halt_S + halt_E + halt_W\n",
    "        return queue_length\n",
    "    \n",
    "    def _get_speed(self):                  # For evaluation \n",
    "        total_speed = 0\n",
    "        car_list = traci.vehicle.getIDList()\n",
    "        for car_id in car_list:\n",
    "            car_speed = traci.vehicle.getSpeed(car_id)\n",
    "            total_speed +=car_speed\n",
    "        if len(car_list) == 0: \n",
    "            s = 0\n",
    "        else: \n",
    "            s = total_speed/len(car_list)\n",
    "        return s\n",
    "            \n",
    "    def _save_episode_stats(self):\n",
    "        \"\"\"\n",
    "        Save the stats of the episode to plot the graphs at the end of the session\n",
    "        \"\"\"\n",
    "        self._reward_store.append(self.reward)  # how much negative reward in this episode\n",
    "        self._speed_store.append(self._sum_speed / self._max_steps)\n",
    "        self._cumulative_wait_store.append(self._sum_waiting_time)  # total number of seconds waited by cars in this episode\n",
    "        self._avg_queue_length_store.append(self._sum_queue_length / self._max_steps)  # average number of queued cars per step, in this episode\n",
    "    \n",
    "    @property\n",
    "    def reward_store(self):\n",
    "        return self._reward_store\n",
    "\n",
    "    @property\n",
    "    def speed_store(self):\n",
    "        return self._speed_store\n",
    "    \n",
    "    @property\n",
    "    def cumulative_wait_store(self):\n",
    "        return self._cumulative_wait_store\n",
    "\n",
    "    @property\n",
    "    def avg_queue_length_store(self):\n",
    "        return self._avg_queue_length_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90af0691",
   "metadata": {
    "id": "90af0691"
   },
   "source": [
    "# 2. Training Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "643c5bc8",
   "metadata": {
    "id": "643c5bc8"
   },
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, Model, Memory, training_epochs, update_epochs, batch_size):\n",
    "        self._model = Model\n",
    "        self._Model_A = Model.critic\n",
    "        self._Model_B = Model.critic_target\n",
    "        self._Memory = Memory\n",
    "        self._loss_store = []\n",
    "        self._batch_size = batch_size\n",
    "        self.history = 0\n",
    "        self.loss_value = 0\n",
    "            \n",
    "    def run(self):\n",
    "        self.training = True\n",
    "        print(\"Training...\")\n",
    "        start_time = timeit.default_timer()\n",
    "        self.loss_value = 0\n",
    "        \n",
    "        for _ in range(training_epochs):\n",
    "            self.experience_replay()\n",
    "            self.loss_value += self.history\n",
    "            if ( _ / update_epochs == 0):\n",
    "                self._model._update_model()\n",
    "        self.loss_value = torch.tensor(self.loss_value)\n",
    "        self._save_loss()\n",
    "        print(\"loss:\",self.loss_value)\n",
    "        training_time = round(timeit.default_timer() - start_time, 1)\n",
    "        \n",
    "        return training_time\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \"\"\"\n",
    "        Retrieve a group of samples from the memory and for each of them update the learning equation, then train\n",
    "        \"\"\"\n",
    "        #batch = self._Memory.get_samples(self._batch_size)  \n",
    "        batch, idxs, is_weights = self._Memory.get_samples(self._batch_size) \n",
    "    \n",
    "        if len(batch) > 0:  # if the memory is full enough\n",
    "            old_states_3 = torch.tensor(([val[0] for val in batch]),dtype=torch.float).to(device) \n",
    "            old_actions_3 = torch.tensor(([val[1] for val in batch]),dtype=torch.int64).to(device)\n",
    "            rewards_2 = torch.tensor(([val[2] for val in batch]),dtype=torch.float).reshape(-1,1).to(device)\n",
    "            rewards_1 = torch.tensor(([val[3] for val in batch]),dtype=torch.float).reshape(-1,1).to(device)\n",
    "            rewards = torch.tensor(([val[4] for val in batch]),dtype=torch.float).reshape(-1,1).to(device)\n",
    "            next_states = torch.tensor(([val[5] for val in batch]),dtype=torch.float).to(device)  \n",
    "            dones = torch.tensor(([val[6] for val in batch]),dtype=torch.float).reshape(-1,1).to(device)\n",
    "            a = torch.tensor(([val[7] for val in batch]),dtype=torch.float).to(device)\n",
    "            b = torch.tensor(([val[8] for val in batch]),dtype=torch.float).to(device)\n",
    "            c = torch.tensor(([val[9] for val in batch]),dtype=torch.float).to(device)\n",
    "            d = torch.tensor(([val[10] for val in batch]),dtype=torch.float).to(device)\n",
    "            weights = torch.tensor((is_weights),dtype=torch.float).reshape(-1,1).to(device)\n",
    "            \n",
    "            ## train the NN  \n",
    "            self.history,TD = self._model.critic_learn(old_states_3,old_actions_3,rewards_2,rewards_1,rewards,next_states,dones,a,b,c,d,weights) \n",
    "            self.history = self.history.to(\"cpu\").squeeze(0).detach() \n",
    "            TD = TD.squeeze(0).detach().tolist()\n",
    "            ## update the priorities \n",
    "            #TD = (self._model.get_TD(old_states_3,old_actions_3,rewards_2,rewards_1,rewards,next_states,dones,a,b,c,d)).squeeze(0).detach().tolist()\n",
    "            \n",
    "            for i in range(len(batch)): \n",
    "                idx = idxs[i]\n",
    "                td = TD[i]   # prior_eps \n",
    "                # s0,a0,r1,s1,done = batch[i]\n",
    "                # TD = self._model.get_TD(s0,a0,r1,s1,0.8,done).squeeze(0).detach().numpy()\n",
    "                self._Memory.update(idx,td)\n",
    "                \n",
    "            self._Model_A.reset_noise()\n",
    "            self._Model_B.reset_noise()\n",
    "                    \n",
    "    def _save_loss(self):\n",
    "        self._loss_store.append(self.loss_value)\n",
    "\n",
    "    @property\n",
    "    def loss_store(self):\n",
    "        return self._loss_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df95f1",
   "metadata": {
    "id": "85df95f1"
   },
   "source": [
    "## Your Deep Reinforcement Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98543acd",
   "metadata": {
    "id": "98543acd"
   },
   "source": [
    "#### Categorical DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3595e2ab",
   "metadata": {
    "id": "3595e2ab"
   },
   "outputs": [],
   "source": [
    "def projection_distribution(ns, r2,r1,r, dones,b,d):\n",
    "    with torch.no_grad():\n",
    "        batch_size  = ns.size(0)\n",
    "        \n",
    "        delta_z = float(Vmax - Vmin) / (num_atoms - 1)\n",
    "        support = torch.linspace(Vmin, Vmax, num_atoms).to(device)\n",
    "\n",
    "        next_action = Model.critic_target(ns,b,d).argmax(1)\n",
    "        next_dist = Model.critic_target.dist(ns,b,d)\n",
    "        next_dist = next_dist[range(batch_size),next_action]\n",
    "\n",
    "        # Tz = rewards + (1 - dones) * gamma * support\n",
    "        Tz = r2+gamma*r1+gamma**2*r + gamma**3 * support * (1-dones)\n",
    "        Tz = Tz.clamp(min=Vmin, max=Vmax)\n",
    "        b  = (Tz - Vmin) / delta_z\n",
    "        l  = b.floor().long()\n",
    "        u  = b.ceil().long()\n",
    "\n",
    "        offset = torch.linspace(0, (batch_size - 1) * num_atoms, batch_size).long()\\\n",
    "                        .unsqueeze(1).expand(batch_size, num_atoms).to(device)\n",
    "\n",
    "        proj_dist = torch.zeros(next_dist.size()).to(device)    \n",
    "        proj_dist.view(-1).index_add_(0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1))\n",
    "        proj_dist.view(-1).index_add_(0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1))\n",
    "        \n",
    "    return proj_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1a46a1",
   "metadata": {
    "id": "ff1a46a1"
   },
   "source": [
    "#### 3-Step TD Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fd772f8",
   "metadata": {
    "id": "8fd772f8"
   },
   "outputs": [],
   "source": [
    "class DRL_Model():\n",
    "    def __init__(self, num_layers, width, batch_size, learning_rate, input_dim, output_dim, tau,num_atoms):\n",
    "        self._i_dim = (3,14,100) #input_dim\n",
    "        self._o_dim = output_dim\n",
    "        self.critic_lr = learning_rate\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.critic = Critic(self._i_dim, width, self._o_dim, num_layers,num_atoms)\n",
    "        self.critic_target = Critic(self._i_dim, width, self._o_dim, num_layers,num_atoms)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr = self.critic_lr)\n",
    "        self._update_model()\n",
    "        self.critic.to(device)\n",
    "        self.critic_target.to(device)\n",
    "     \n",
    "    ## DQN Algorithm \n",
    "    def critic_learn(self,s3,a3,r2,r1,r,ns,done,a,b,c,d,weights):    \n",
    "        # y_pred = torch.zeros(self.batch_size,1)\n",
    "        # y_true =  r2+gamma*r1+gamma**2*r+gamma**3*torch.max(self.critic_target(ns,b,d)).detach()       #*(1-done) ).detach()   # y_true \n",
    "        # a3 = torch.tensor(a3,dtype=torch.int64).reshape(-1,1)\n",
    "        # y_pred = torch.gather(self.critic(s3,a,c),1,a3)\n",
    "        # loss_fn = nn.MSELoss()\n",
    "        # loss = (weights * loss_fn(y_pred, y_true)).mean()\n",
    "        \n",
    "        proj_dist = projection_distribution(ns, r2,r1,r, done,b,d)\n",
    "        dist = self.critic.dist(s3,a,c) \n",
    "        log_p = torch.log(dist[range(batch_size),a3]) \n",
    "        elementwise_loss = - (Variable(proj_dist) * log_p).sum(1)\n",
    "        TD = elementwise_loss.detach()\n",
    "        loss = torch.mean(weights * elementwise_loss) \n",
    "        \n",
    "        self.critic_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.critic.parameters(), 10.0)\n",
    "        self.critic_optim.step()\n",
    "        return loss,TD\n",
    "    \n",
    "    def get_TD(self,s3,a3,r2,r1,r,ns,done,a,b,c,d, alpha=0.6):        # for priority experience replay \n",
    "        s3 = torch.tensor(s3,dtype=torch.float).reshape(-1,3,14,100).to(device)\n",
    "        ns = torch.tensor(ns,dtype=torch.float).reshape(-1,3,14,100).to(device)\n",
    "        a = torch.tensor(a,dtype=torch.float).to(device)\n",
    "        b = torch.tensor(b,dtype=torch.float).to(device)\n",
    "        c = torch.tensor(c,dtype=torch.float).to(device)\n",
    "        d = torch.tensor(d,dtype=torch.float).to(device)\n",
    "        done = torch.tensor(done,dtype=torch.float).reshape(-1,1).to(device)\n",
    "        a3 = torch.tensor(a3,dtype=torch.int64).reshape(-1,1).to(device)\n",
    "        r2 = torch.tensor(r2,dtype=torch.float).to(device)\n",
    "        r1 = torch.tensor(r1,dtype=torch.float).to(device)\n",
    "        r = torch.tensor(r,dtype=torch.float).to(device)\n",
    "        \n",
    "        #y_true = r2+gamma*r1+gamma**2*r+gamma**3*torch.max(self.critic_target(ns,b,d))#*(1-done)   # y_true \n",
    "        #y_pred = torch.zeros(self.batch_size,1)\n",
    "        #y_pred = torch.gather(self.critic(s3,a,c),1,a3)\n",
    "        #TD = abs(y_true - y_pred).detach() \n",
    "        \n",
    "        proj_dist = projection_distribution(ns, r2,r1,r, done,b,d)\n",
    "        dist = self.critic.dist(s3,a,c)\n",
    "        \n",
    "        action = a3.unsqueeze(1).expand(s3.shape[0], 1, num_atoms)\n",
    "        dist = dist.gather(1, action).squeeze(1)\n",
    "        dist.data.clamp_(0.01, 0.99)\n",
    "        log_p = dist.log() \n",
    "        #log_p = torch.log(torch.gather(self.critic.dist(s3,a,c),1,a3))\n",
    "        \n",
    "        elementwise_loss = - (Variable(proj_dist) * log_p).sum(1)\n",
    "        elementwise_loss = elementwise_loss.mean()\n",
    "        TD = elementwise_loss\n",
    "        \n",
    "        return TD\n",
    "    \n",
    "    def predict(self, s,a,c):\n",
    "        s = torch.tensor(s, dtype=torch.float).unsqueeze(0).to(device)\n",
    "        a = torch.tensor(a,dtype=torch.float).to(device)\n",
    "        c = torch.tensor(c,dtype=torch.float).to(device)\n",
    "        return self.critic(s,a,c).squeeze(0).detach()\n",
    "        \n",
    "    def _update_model(self):\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_target.eval()\n",
    "                                       \n",
    "    def soft_update(self,net_target, net):\n",
    "        for target_param, param  in zip(net_target.parameters(), net.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "        self.critic_target.eval()\n",
    "        \n",
    "    def _load_my_model(self, model_folder_path):\n",
    "        \"\"\"\n",
    "        Load the model stored in the folder specified by the model number, if it exists\n",
    "        \"\"\"\n",
    "        model_file_path = os.path.join(model_folder_path, 'trained_model.h5')\n",
    "        \n",
    "        if os.path.isfile(model_file_path):\n",
    "            loaded_model = torch.load(model_file_path)\n",
    "            self.critic = loaded_model\n",
    "            print(\"Load model successfully!\")\n",
    "        else:\n",
    "            sys.exit(\"Model number not found!\")\n",
    "            \n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Save the current model in the folder as h5 file and a model architecture summary as png\n",
    "        \"\"\"\n",
    "        torch.save(self.critic, os.path.join(path, 'trained_model.h5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5c4b33",
   "metadata": {
    "id": "6d5c4b33"
   },
   "source": [
    "# 3. Run and Result Display "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b8d5edb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "7b8d5edb",
    "outputId": "2669f605-1d74-48a3-a811-af73db1a5c79",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d2df83bb9f4c>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0msimulation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrafficGen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msumo_cmd\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgreen_duration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myellow_duration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mTraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mupdate_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mepisode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Training' object is not callable"
     ]
    }
   ],
   "source": [
    "from generator import TrafficGenerator\n",
    "from memory import Memory     ## Prority Experience Memory \n",
    "from visualization import Visualization\n",
    "from utils import import_train_configuration,set_sumo, set_train_path,get_model_path\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = import_train_configuration(config_file='training_settings.ini')\n",
    "    sumo_cmd = set_sumo(gui, config['sumocfg_file_name'], max_steps)\n",
    "    path = set_train_path(config['models_path_name'])\n",
    "    model_path = get_model_path(config['models_path_name'], model_to_test)\n",
    "\n",
    "    Model = DRL_Model(num_layers, width_layers, batch_size, learning_rate, input_dim=num_states, output_dim=num_actions,tau=1,num_atoms=num_atoms)\n",
    "\n",
    "    memory = Memory(memory_size_max, memory_size_min)\n",
    "\n",
    "    TrafficGen = TrafficGenerator(max_steps, n_cars_generated)\n",
    "\n",
    "    Visualization = Visualization(path, dpi=96)\n",
    "        \n",
    "    simulation = Simulation(Model,memory,TrafficGen,sumo_cmd,gamma,max_steps,green_duration,yellow_duration,num_states,num_actions)\n",
    "  \n",
    "    Training = Training(Model,memory,training_epochs,update_epochs,batch_size)\n",
    "    \n",
    "    episode = 0\n",
    "    timestamp_start = datetime.datetime.now()\n",
    "     \n",
    "    # Model._load_my_model(model_path)\n",
    "    \n",
    "    while episode < total_episodes:\n",
    "        print('\\n----- Episode', str(episode+1), 'of', str(total_episodes))\n",
    "        epsilon = (1.0 - ((episode+1) / total_episodes))  # set the epsilon for this episode according to epsilon-greedy policy\n",
    "        #epsilon = 0\n",
    "        simulation_time = simulation.run(episode, epsilon)  # run the simulation\n",
    "        training_time = Training.run()                      # train the model  \n",
    "        print('Simulation time:', simulation_time, 's - Training time:', training_time, 's - Total:', round(simulation_time+training_time, 1), 's')\n",
    "        episode += 1\n",
    "        \n",
    "    print(\"\\n----- Start time:\", timestamp_start)\n",
    "    print(\"----- End time:\", datetime.datetime.now())\n",
    "    print(\"----- Session info saved at:\", path)\n",
    "\n",
    "    Model.save_model(path)\n",
    "\n",
    "    copyfile(src='training_settings.ini', dst=os.path.join(path, 'training_settings.ini'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1d04f",
   "metadata": {
    "id": "e7d1d04f"
   },
   "outputs": [],
   "source": [
    "Visualization.save_data_and_plot(data=Simulation.reward_store, filename='reward', xlabel='Episode', ylabel='Cumulative negative reward')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578e3c3b",
   "metadata": {
    "id": "578e3c3b"
   },
   "outputs": [],
   "source": [
    "Visualization.save_data_and_plot(data=Simulation.cumulative_wait_store, filename='delay', xlabel='Episode', ylabel='Cumulative delay (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7c57e4",
   "metadata": {
    "id": "8e7c57e4"
   },
   "outputs": [],
   "source": [
    "Visualization.save_data_and_plot(data=Simulation.avg_queue_length_store, filename='queue', xlabel='Episode', ylabel='Average queue length (vehicles)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc6cc1",
   "metadata": {
    "id": "1fcc6cc1"
   },
   "outputs": [],
   "source": [
    "Visualization.save_data_and_plot(data=Training.loss_store, filename='Loss', xlabel='Episode', ylabel='Total Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37846a20",
   "metadata": {
    "id": "37846a20"
   },
   "outputs": [],
   "source": [
    "Visualization.save_data_and_plot(data=Simulation.speed_store, filename='Speed', xlabel='Episode', ylabel='Average Speed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7727e7c7",
   "metadata": {
    "id": "7727e7c7"
   },
   "source": [
    "- Categorical \n",
    "- Action definition \n",
    "- Phase Gate, FRAP \n",
    "- Memory Palace for Change-Stay\n",
    "- Run 3 * 4 model\n",
    "- not too difference with dones"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
